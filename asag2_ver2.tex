\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{A Platform for Searching Texts for Desired Expressions in a User-Editable Pattern Matching Environment for Language Learning\\
%{\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and
%should not be used}
%\thanks{Identify applicable funding agency here. If none, delete this.}
}

%author情報追記
\author{\IEEEauthorblockN{Tatsuya Katsura}
\IEEEauthorblockA{\textit{Okayama University, Japan}\\
pcn05fd3@s.okayama-u.ac.jp}
\and
\IEEEauthorblockN{Koichi Takeuchi}
\IEEEauthorblockA{\textit{Okayama University, Japan}\\
\textit{takeuc-k@okayama-u.ac.jp}}
}



\maketitle

\begin{abstract}
In this paper we propose a platform of pattern matching system that can extract required phrases or sentences in texts.
Finding certain expressions in texts are often needed in language learning, e.g., examples of case markers
between a predicate and an argument, or possible nouns in subject of a verb in a certain meaning. In previous studies,
several types of systems, containing concordancers, are proposed. However, it is not easy to apply combined
patterns because the pattern matching templates are previously fixed.
% flexibilityが必要 背景としては task specific な systemで作られているので 英語検索に特化はしてると
% 小論文での表現でみんなが同じようにどう使っているかといった表現の分析に利用するのは容易ではない
% その一番の原因は flexibility 
Thus, we propose a flexible phrase searching system in which users can create search patterns
by combining blocks of basic search templates. % 拡張性がある blockで　提案する
%長すぎる時はここを消す
One of the characteristics the proposed system is that user can also specify where to be highlighted
in texts with the blocks.
To realize the function of combining patterns by users, the proposed system employs
Prolog as the base of the data structure. The platform of the searching system is implemented
on an Web server with JavaScript-based interface and database system. 
% ブロックにいろいろ設定することで，ユーザは組み合わせてさらに使うことができる
%To realize the pattern matching of predicate-argument structure, 
%the system emplopys several NLP tools.
In the performance test, we show that the proposed system can deal with relatively large scale texts
(10,000 sentences), and also demonstrate the combined patterns can be applied to the texts.
In this paper we discuss the system architecture and the extendability of the pattern matching. 
\end{abstract}

\begin{IEEEkeywords}
  pattern matching, concordancer, browser-based pattern matcher, Prolog
\end{IEEEkeywords}

\section{Introduction}
Extraction of some phrases and expressions in texts is considered essential function in language education.
For example, case markers located between a predicate and an object in Japanese language are various
rules and alternations\footnote{The verbs {\it morau} (get) and {\it eru} (obtain) are almost the same meaning
but {\it morau} has alternation between the case markers {\it ni/kara} as {\it kare ni/kara morau}
((I) get (it) from him.), and 
{\it eru} can only takes {\it kara} in this meaning as {\it kare kara/*ni eru}.},
% 彼 から もらう，彼に もらう but, 彼から 得る 彼に 得る.
% もらう，得るの質問 native https://ja.hinative.com/questions/17482159
then language learners need to search for examples of predicate-argument
examples in Japanese texts. 
%also, search for similar phrases in student essays. this is example for use in education. 
As a tool for searching texts, various kinds of concordancer are proposed, however, 
most concordancers have the limitations in the use of their functionality.
For example, Sketch Engine\footnote{https://www.sketchengine.eu/}, one of the famous concordancers,
proposes Corpus Query Language (CQL)\footnote{https://www.sketchengine.eu/documentation/corpus-querying/}
that has rich templates of pattern matching for words and characters, however, 
most of the templates are mainly effective for English and the templates are realized 
at the level of regular expressions. Thus, users are unable to employ patterns at the level of Context
Free Grammar (CFG) which incorporates dependency parsing or predicate-argument relations.
%predicate object relations with variations of case markers, On the other hand,
From the viewpoint of Natural Language Processing (NLP) research,
dependency parsers (e.g., KNP\footnote{https://nlp.ist.i.kyoto-u.ac.jp/?KNP}, CaboCha\footnote{https://taku910.github.io/cabocha/}, GiNZA\footnote{https://megagonlabs.github.io/ginza/}) have already been developed and are available, but it is not easy to
build a pattern matching system with NLP tools from scratch. 

Thus, we propose an environment that users can combine patterns
searching for phrases or expressions in texts without programming.
The system allows users to combine the basic search templates
connected with Prolog predicates that have all of the information
about dependency, part of speech (POS), and lemmas of sentences analyzed by NLP tools. 
The combination of search templates are consistently composed and the
combined search pattern works to find the intended phrases in texts thanks to
the Prolog inference engine. 
The system has a browser-based interface based on Blockly\footnote{https://developers.google.com/blockly?hl=ja}
that allows users to combine patterns in a visual environment. 

The contributions of this study are as below:
1) we propose a new type pattern matching system providing the environment in which users can
combine patterns, 
2) Prolog-based search templates allow users flexibility of pattern combinations, and 
3) browser-based system that can be suitable for non-programmers.
In this paper we focus on Japanese pattern matching system because 
most of the concordancers are build for English, and the target of the system is
not only for language learners but also for analyzing texts to check the student essays. 

In the performance test, we shows that the proposed system can deal with relatively large scale texts
(10,000 sentences), and also demonstrate the combined patterns can be applied to the texts.
In this paper we discuss the system architecture and the flexibility of the pattern matching. 

\section{Related Work}

% sketch engine
One of the famous concordancers is Sketch Engine. Sketch Engine provide an environment
that user can search for inflections, lemmas, words, part-of-speeches (POSes), and more
with the CQL that works at the level of regular grammar.
Sketch Engine offers a Japanese search environment as well\footnote{https://www.sketchengine.eu/corpora-and-languages/japanese-text-corpora/}; however, the search templates
based on Japanese morphological analyzers appear a little simple and
do not cover dependency parsing

Other than concordancers, a corpus management system ChaKi.NET
\cite{asahara2016} has been constructed for Japanese dependency
structure annotated corpora. Chaki.NET provides several search
templates that can capture dependency structure; but the functions are
limited and aimed at annotation task. Thus, there is no function such
as combining patterns.

% Npcmj tregex 
As a tool that provides search function for NINJAL Parsed Corpus of Modern Japanese (NPCMJ)
\footnote{https://npcmj.ninjal.ac.jp/index.html}\cite{Horn-et-al-2018},
NPCMJ Explorer\footnote{https://npcmj.ninjal.ac.jp/explorer/} has been developed. 
The Explorer is implemented based on Tregex \cite{roger2006} that can extract designated phrases or words form
parsed tree structure\footnote{https://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/trees/tregex/
TregexPattern.html}. The possible search patterns are effective and the several combinations of pattern
are available; however, the users are required to build a search formula to capture the desired expressions\footnote{For example, the pattern ``\_\_ !$<$ \_\_'' indicates extraction of words from the tree corpus.
As this pattern shows, users are requires to have enough knowledge of pattern definitions to build
a combined pattern. Thus, 
frequently used patterns are already defined and available to users in NPCMJ Explorer.}

% text mining
In NLP study, StruAP\cite{yanai2017} which is a pattern matching tool for parsed trees is proposed.
StruAP can designate complex combination of nodes in syntax trees and has a Web-based interface. 
The patterns, however, should be defined by the pattern language,
and the tool is a commercial product and not publicly available. 
Thus, we cannot use the tools. 


\section{Platfrom of Pattern Matching System}
To realize the environment that users can edit patterns for extracting desired expressions
with visual operations, the system is composed of a front-end system and a back-end system. 
The front-end system provides a visual interface on which user can conduct all of the operations
such as uploading text files, editing patterns and confirming the extracted results. 
On the other hand, the back-end system provides all of the text processing tasks such as
saving the uploaded texts, analyzing texts with NLP tools, and returning results of search texts
with the pattern requested from the front-end. 

Since the back-end system is independent, it is possible to incorporate several NLP tools
and use them from the front-end. The composed patterns by users can be saved and reused by
the other users. Thus, the users can concentrate on creating patterns that match their requests,
and obtain the search results. 

%In this paper we construct a matching system for Japanese texts, thus, in the following, we explain the details of
%Japanese text matching system.

This paper describes the development of a matching system for Japanese texts.
The following sections provide detailed explanations of the Japanese text matching system.



\subsection{Overview of the Pattern Matching System} % workflow architecture 
Figure \ref{fig:sys} shows that the overview of the proposed pattern matching system.
The encircled numbers indicate the number of steps in the workflow of pattern matching.
The front-end system is a JavaScript-based interface which has the functions of
uploading texts, showing the results of analyzed texts, composing patterns, and showing the results of
matched phrases with texts. The block-based environment is implemented using Blockly.
For the management of composed patterns, the frond-end system has also
the functions of downloading and uploading composed patterns. 

\begin{figure}[htbp]
%\centerline{\includegraphics[scale=.4]{fig/system.eps}}
\caption{Overview of the pattern matching system.}
\label{fig:sys}
\end{figure}

The back-end composed of the three modules, the Python-based NLP processing module,
Prolog processing module implemented using SWI Prolog\footnote{https://www.swi-prolog.org/},
and database system implemented using Elasticsearch\footnote{https://www.elastic.co/jp/elasticsearch/}.

According to the workflow, after texts are uploaded at the front-end,
morphological analysis \footnote{https://taku910.github.io/mecab/} and
dependency parsing \footnote{https://taku910.github.io/cabocha/}, and
argument structure analysis\footnote{https://github.com/Takeuchi-Lab-LM/python\_asa}
\cite{take2011f} are applied into the texts at the NLP module.
By this processing, the texts are segmented into morphemes and
chunks with grammatical and semantic tags. 
All of the data are sent to the Prolog module and then, they are converted
to the Prolog predicates. The predicates are stored in the database at the database module.

After the user submits the search patterns, i.e., queries in Prolog format using block-based JavaScript,
the system extracts Prolog predicates corresponding to each sentence from the database, and executes the
pattern matching of Prolog on the predicates with the queries. 

The results of pattern matching are sent from the back-end to the front-end.
The matched expressions are displayed in red in the text.
This function can be realized because the position information is recorded 
using Prolog predicates. The details of format of the predicates
in this task are described in Section \label{sec:prolog}. 


\subsection{Prolog-based Data Structure} 
\label{sec:prolog}
The assumed demands of searching texts are extracting chunk, phrases and words with 
combination of constrains specifying dependencies of subjects and predicates, as well as particles and POSes.
To meet these demands, texts are segmented to morphemes with lemmas and POSes using the morphological analyzer, and
the morphemes are grouped into chunks added with dependency relations using the dependency parser.
After the chunks are analyzed their predicate-argument relations, and semantic roles \footnote{We
currently implemented extended thematic roles\cite{fillmore68,take2011f} such as 動作主 (Agent), 対象 (Theme), 時間 (Goal) and so on. The details are in Predicate Thesaurus https://pth.cl.cs.okayama-u.ac.jp/. } are assigned to the arguments of the predicates. 

To reflect the rich information to make if available for search, we define the predicates
based on the relations between nodes. Figure \label{fig:tree} depicts a tree structure of
a sentence with analyzed results using NLP tools. 

\begin{figure}[htbp]
%\centerline{\includegraphics[scale=.4]{fig/tree.eps}}
\caption{Tree structure of a sentence with analyzed results using NLP tools.} 
\label{fig:tree}
\end{figure}

Thus, we define the following Prolog predicates to capture the tree structure of
the analyzed results Table \ref{tbl:predicates}. 

\begin{table}[htbp]
\caption{Prolog Predicates}
\begin{center}
  \begin{tabular}{|l|l|}\hline
      Predicates &  Descrption \\\hline
      chunk(SentID, 0, ChunkID) & chunk IDs  \\
      morph(SentID, ChunkID, MorphID) & morpheme IDs \\  
      main(SentID, ChunkID, String) & main morpheme in the chunk\\
      part(SentID, ChunkID, String) & particle in the chunk \\
      role(SentID, ChunkID, String) & semantic role in the chunk \\
      semantic(SentID, ChunkID, String) & sense of the predicate \\
      surf(SentID, NodeID, String) & surface string for the node\\
                       & ID \\
      surfBF(SentID, MorphID, String) & base form or lemma \\
      sloc(SentID, Morph/ChunkID, Position) & position of the chunk\\
      pos(SentID, MorphID, String) & part of speech for the \\
                                   &  morpheme\\
      dep(SentID, ChunkID, ChunkID) & dependent relation between \\
                        & chunks\\\hline
\end{tabular}
\label{tbl:predicates}
\end{center}
\end{table}
In the table, the {\it NodeID} in the predicate {\it surf} denotes
morpheme ID, chunk ID, and 0 that indicates sentence node.
The morpheme IDs and chunk IDs are set so that they do not overlap,
Thus, their IDs are unique. The morpheme ID starts after the last number
of chunk. Then the predicate {\it surf} can
record a surface string at any node.

In the predicate {\it sloc}, the argument {\it Position} denotes
the position that consists of start and end positions that express number of characters
from the beginning of the sentence.
For example, the position of the first chunk ``生徒が'' in Figure \ref{fig:tree}
is ``0\_2''. This provides the position of the string that are matched to the pattern.



\subsection{Example of Query}
\label{sec:example}
In the previous section, we defined the predicates in database for recording tree structure.
Using the predicates defined in the previous section, 
users can build a search query. All of the predicates are connected to the blocks
in Blockly. Figure \ref{fig:pattern} shows that an example of prolog-based query to extract
to extract predicates and arguments that has dependency relation with the case marker ``を''
(direct object case).

\begin{figure}[htbp]
%\centerline{\includegraphics[scale=.4]{fig/wo.eps}}
\caption{Example of a search query: to extract predicates and arguments that has dependency relation with the case marker ``を'' (direct object case).}
\label{fig:pattern}
\end{figure}

In Figure \ref{fig:pattern}, the dark blue blocks denote the prolog predicates that are defined
in the database. The purple block denotes the user defined predicates. Since all of the blocks
follow the Prolog format, they can be combined to make more complicate patterns based on the user define blocks.

All of the search results are recorded to the variables whose name is starting a capital character.
In the query of Figure \ref{fig:pattern}, the arguments {\it Wo} and {\it Verb} are intended
to extract source of dependent and its verb, respectively.
The {\it Wo\_Sloc} and {\it Verb\_Sloc} denotes the position information and the system
will highlight the parts when the user designates the {\it \_Slock} variable. 
All of the predicates such as {\it pos}, {\it part} and {\it morph} works a conjunctions.
For example, the predicate ``pos(SENTENCE\_ID, Verb\_morph\_id, 動詞)'' indicates that
the predicate {\it pos} has three arguments and the third arguments are fixed as ``動詞'' (verb).
Thus, this predicate matches only those whose POSes are verbs; then the variables
{\it SENTENCE\_ID} and {\it Verb\_morph\_id} will record the IDs by unification. 


In the displaying the results. the system has three display modes, i.e.,
table, highlight and keyword in context (KWIC).
Figure \ref{fig:table} shows the results of matched chunks with table format. 

\begin{figure}[htbp]
%\centerline{\includegraphics[scale=.4]{fig/table.eps}}
\caption{Displaying results of pattern matching: all variables.}
\label{fig:table}
\end{figure}

In the table, the variables {\it Wo} and {\it Wo\_Sloc} are extracted
independently. Thus, the user can designate which words or chunks should be highlighted in
the highlight mode using the {\it \_Sloc} variable (Figure \ref{fig:show1}). 

\begin{figure}[htbp]
%\centerline{\includegraphics[scale=.4]{fig/show1.eps}}
\caption{Displaying the results of pattern matching: highlighting the argument {\it Wo\_Sloc}.}
\label{fig:show1}
\end{figure}

Figure \ref{fig:show1} shows the highliting {\it Wo\_Sloc} parts in the texts\footnote{The duplicated
sentences are displayed due to the bidirectional dependencies settings.}.
The user can also see the highighting of the verb variables by selecting the {\it Verb\_Sloc} (Figure \ref{fig:showv}). 

\begin{figure}[htbp]
%\centerline{\includegraphics[scale=.4]{fig/showv.eps}}
\caption{Displaying the results of pattern matching: highlighting the argument {\it Verb\_Sloc}.}
\label{fig:showv}
\end{figure}

The flexibility of highlight function displaying must be an advantage for checking students' essays as well as
learning languages. 

\section{Performance Tests}
\label{sec:experiment}
The system stores all of the texts to database in Elasticsearch. Thus,
the system is expected to work for the large scale texts; however, the
system works rather slow because the current implementation does not
focus on speed but focuses on achieving consistent behavior.  As a
performance test experiment, we evaluate the processing time when
inputting a little large size of text, such as 10,000 lines.

As a result of the experiment, it took about six minutes to upload the text.
The task of upload consists of sending texts to servers, applying NLP tools
to the texts, converting all information to Prolog predicates and storing them
to the database. 
The time to run the pattern match on the text was about five minutes.
The task of pattern match consists of extracting each predicates corresponding to the sentence,
executing Prolog matching, and sending all of the results from the back-end to the front-end. 
Thus, the system works on the 10,000 lines of text but it takes a little long time to run
the pattern matching. 

\section{Discussions and Conclusions}
The proposed system can extract required phrases or sentences in texts with the user-editable
pattern matching environment. The function of consistent pattern combinations depends on the Prolog format.
While the pattern combination is flexible, it can be somewhat difficult for users to build patterns
because users may be not familiar with the Prolog format.

As described in Section \ref{sec:example},
the system requires special variables with {\it \_Sloc} suffix that indicate the position of the
target expressions. This feature have both advantages and disadvantages. The position variable
provides a flexibility of selection of highlight part, however, it may seem to be redundant for
users who want easier operation. Thus, it may be necessary to prepare a predicate that hides
position variable. 

In the current implementation, the patching system does not accept ambiguous matching such as ``*'';
but the SWI-Prolog provides a function to deal with ambiguous matching of regular expressions. 
This must be an issue for the future.

%\section*{Acknowledgments}
%This work was partially supported by JSPS KAKENHI Grant Number 22K00530.





\bibliographystyle{unsrt}

%\bibliography{all,my-results}

%\begin{thebibliography}{00}
%\end{thebibliography}

\end{document}
